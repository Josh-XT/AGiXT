version: "3.8"
services:
  frontend:
    build: ./frontend
    init: true
    env_file:
      - .env
    environment:
      NEXT_PUBLIC_API_URI: ${NEXT_PUBLIC_API_URI:-http://backend:7437}
    ports:
      - "127.0.0.1:3000:3000"
    depends_on:
      - backend

  fastchat:
    build:
      dockerfile: Dockerfile-fastchat
    environment:
      FASTCHAT_DEVICE: "cpu"
      MODEL_NAME: "vicuna-7b-1.1"
      MODEL_PATH: "/model/vicuna-7b-1.1"
      # or to use another model:
      # MODEL_NAME: "fastchat-t5-3b-v1.0"
      # MODEL_PATH: "/model/fastchat-t5-3b-v1.0"
    ports:
      - "127.0.0.1:21001:21001"
      - "127.0.0.1:21002:21002"
      - "127.0.0.1:7500:7500"
    volumes:
      # - /local/path/to/llama-7b-hf:/model/llama-7b-hf
      - /local/path/to/vicuna-7b-1.1:/model/vicuna-7b-1.1
      # - /local/path/to/fastchat-t5-3b-v1.0:/model/fastchat-t5-3b-v1.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  backend:
    build:
      context: .
      dockerfile: Dockerfile-backend
    init: true
    env_file:
      - .env
    ports:
      - "127.0.0.1:7437:7437"
    volumes:
      - ./data/agents:/app/agents:rw
      - ./data/workspace:/app/WORKSPACE:rw
      # - ./models:/model:rw
    depends_on:
      - fastchat
